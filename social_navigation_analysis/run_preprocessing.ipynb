{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, datetime, warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pycircstat\n",
    "\n",
    "warnings.filterwarnings('ignore') # ignore all!\n",
    "# or: catch specific warnings\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "\n",
    "# directory to social_navigation_analysis:\n",
    "user = os.path.expanduser('~')\n",
    "sys.path.insert(0, '..')\n",
    "sys.path.insert(0, str(Path(f'{user}/Dropbox/Projects/social_navigation_analysis/social_navigation_analysis'))) \n",
    "import preprocess as snt_preprc\n",
    "from preprocess import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets: ['CUD' 'HC' 'PD' 'PTSD' 'prolific_initial_vt' 'prolific_initial'\n",
      " 'prolific_replication' 'schema_day01' 'schema_day03' 'adolescent_pilot']\n"
     ]
    }
   ],
   "source": [
    "main_dir = f'{user}/Desktop/SNT_data'\n",
    "datasets = pd.read_excel(f'{main_dir}/SNT-datasets.xlsx') \n",
    "print(f\"All datasets: {datasets['sample'].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUD: found 80 files\n",
      "Finished parsing & computing behavior with 0 errors\n",
      "There are missing organized excel files\n",
      "Summarizing behavior\n",
      "HC: found 21 files80\n",
      "Creating subdirectory for behavior\n",
      "Finished parsing & computing behavior with 0 errors\n",
      "There are missing organized excel files\n",
      "Summarizing behavior\n",
      "PD: found 61 files21\n",
      "Creating subdirectory for behavior\n",
      "Finished parsing & computing behavior with 0 errors\n",
      "There are missing organized excel files\n",
      "Summarizing behavior\n",
      "PTSD: found 58 files\n",
      "Creating subdirectory for behavior\n",
      "Finished parsing & computing behavior with 0 errors\n",
      "There are missing organized excel files\n",
      "Summarizing behavior\n",
      "Summarizing 58 of 58\r"
     ]
    }
   ],
   "source": [
    "# Usage notes:\n",
    "# expects & creates some folder structure like:\n",
    "    # SNT/Raw_files\n",
    "    # SNT/Organized\n",
    "    # SNT/Behavior\n",
    "    # SNT/Posttask\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# convenience functions\n",
    "\n",
    "def find_files(directory):\n",
    "    return [f for f in glob.glob(f\"{directory}/*\") if '~$' not in f]\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "# dataset info\n",
    "# synapse_dir = '/Volumes/synapse/projects/SocialSpace/Projects/'\n",
    "# datasets    = pd.read_excel(f'{synapse_dir}/SNT-datasets.xlsx') \n",
    "\n",
    "main_dir = f'{user}/Desktop/SNT_data'\n",
    "datasets = pd.read_excel(f'{main_dir}/SNT-datasets.xlsx') \n",
    "samples  = ['CUD','HC','PD','PTSD']\n",
    "\n",
    "# print(f\"All datasets: {datasets['sample'].values}\")\n",
    "# print(f'Selected datasets: {samples}')\n",
    "\n",
    "# what to do\n",
    "overwrite   = False\n",
    "compute_beh = True # right now: both parsing & behavior; TODO: split up\n",
    "summarize   = True\n",
    "\n",
    "for sample in samples:\n",
    "\n",
    "    #---------------------------\n",
    "    # define the project details\n",
    "    #---------------------------\n",
    "\n",
    "    proj_ix     = np.where(datasets['sample'].values==sample)[0][0]\n",
    "    project_dir = f\"{datasets.loc[proj_ix,'raw_directory']}\"\n",
    "    file_format = datasets.loc[proj_ix,'raw_format']\n",
    "    datasets.loc[proj_ix, 'last_processed'] = datetime.datetime.now()\n",
    "    raw_dir    = f\"{main_dir}/{datasets.loc[proj_ix, 'raw_directory']}\"\n",
    "    \n",
    "\n",
    "    if compute_beh: \n",
    "\n",
    "        # find files\n",
    "        raw_files = find_files(raw_dir)\n",
    "        print(f'{sample}: found {len(raw_files)} files')\n",
    "        errors = []\n",
    "        for raw_fname in raw_files:\n",
    "            \n",
    "            #------------------------------------------------\n",
    "            # parse the raw file into an organized file\n",
    "            #------------------------------------------------\n",
    "\n",
    "            fname = raw_fname.split('/')[-1]\n",
    "        \n",
    "            try:\n",
    "\n",
    "                print(f'{fname}: parsing raw', end='\\r')\n",
    "\n",
    "                # TODO check sub_id - does it exist already?\n",
    "\n",
    "                if file_format == 'log':\n",
    "                    xlsx_fname = snt_preprc.parse_log(raw_fname, experimenter=datasets.loc[proj_ix,'experimenter'], output_timing=False, out_dir=f'{raw_dir}/..')\n",
    "                elif file_format == 'csv':\n",
    "                    xlsx_fname = snt_preprc.parse_csv(raw_fname, snt_version=datasets.loc[proj_ix,'options_version'], out_dir=f'{raw_dir}/..')\n",
    "                elif file_format == 'txt': # txt -> csv -> xlsx\n",
    "                    csv_raw_dir = raw_dir.replace('-txt', '-csv')\n",
    "                    raw_fname   = snt_preprc.format_txt_as_csv(raw_fname, out_dir=csv_raw_dir)\n",
    "                    xlsx_fname  = snt_preprc.parse_csv(raw_fname, snt_version=datasets.loc[proj_ix,'options_version'], out_dir=f'{csv_raw_dir}/..')\n",
    "\n",
    "            except: \n",
    "\n",
    "                errors.append(f'Parsing raw: {raw_fname}')\n",
    "        \n",
    "            #------------------------------------------\n",
    "            # compute behavior from organized file \n",
    "            #------------------------------------------\n",
    "            \n",
    "            if sample != 'schema_day03': # doesnt have snt \n",
    "            \n",
    "                try: \n",
    "\n",
    "                    if xlsx_fname is not None:\n",
    "                        \n",
    "                        fname = xlsx_fname.split('/')[-1]\n",
    "                        print(f'{fname}: computing behavior', end='\\r')          \n",
    "                        snt_preprc.compute_behavior(xlsx_fname, weight_types=False, decision_types=False, coord_types=False, out_dir=f'{raw_dir}/..')\n",
    "                \n",
    "                except: \n",
    "                    \n",
    "                    errors.append(f'Computing behavior: {raw_fname}')\n",
    "\n",
    "\n",
    "            #------------------------------------------\n",
    "            # compute rdvs for mvpa analyses\n",
    "            #------------------------------------------\n",
    "            # \n",
    "            # \n",
    "            # \n",
    "            #\n",
    "\n",
    "\n",
    "        print(f'Finished parsing & computing behavior with {len(errors)} errors')\n",
    "\n",
    "        # check number of files:\n",
    "        n_xlsx = find_files(f\"{raw_dir}/../Organized\")\n",
    "        if len(raw_files) != n_xlsx: print('There are missing organized excel files')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #------------------------------\n",
    "    # summarize across subjects\n",
    "    #------------------------------  \n",
    "    \n",
    "    if summarize: \n",
    "\n",
    "        if sample != 'schema_day03': # doesnt have snt\n",
    "\n",
    "            print('Summarizing behavior')\n",
    "\n",
    "            # n_timing    = len([f for f in glob.glob(f\"{project_dir}/{datasets.loc[proj_ix, 'timing_directory']}/*\") if '~$' not in f])\n",
    "            # datasets.loc[proj_ix, 'n_raw_files']       = n_raw\n",
    "            # datasets.loc[proj_ix, 'n_organized_files'] = n_xlsx\n",
    "            # datasets.loc[proj_ix, 'n_timing_files']    = n_timing\n",
    "            # datasets.loc[proj_ix, 'n_behavior_files']  = n_behavior\n",
    "\n",
    "            # check number of files:\n",
    "            xlsx_files  = find_files(f\"{raw_dir}/../Organized\")\n",
    "            behav_files = find_files(f\"{raw_dir}/../Behavior\")\n",
    "            if len(xlsx_files) != len(behav_files): print(f'There are missing behavioral files: {len(behav_files)}!={len(xlsx_files)}')\n",
    "            snt_preprc.summarize_behavior(behav_files, out_dir=f'{raw_dir}/..')\n",
    "\n",
    "        # if there is a posttask folder concatenate autoamatically and output:\n",
    "        if os.path.exists(f'{raw_dir}/../Posttask'):\n",
    "            \n",
    "            print('Summarizing posttask')\n",
    "\n",
    "            post_files = find_files(f'{raw_dir}/../Posttask')\n",
    "            if sample != 'schema_day03':\n",
    "                if len(post_files) != len(behav_files): print(f'There are missing posttask files: {len(post_files)}!={len(behav_files)}')\n",
    "\n",
    "            post_df = pd.concat([pd.read_excel(f) for f in post_files], axis=0)\n",
    "            post_df.rename(columns={'Unnamed: 0':'sub_id'}, inplace=True)\n",
    "            post_df.to_excel(f'{raw_dir}/../SNT-posttask_n{len(post_df)}.xlsx', index=False)\n",
    "\n",
    "            # TODO: add in dots, self-reports, posttask etc automatically... eg, check if there is another summary sheet available, add in as an argument to merge on sub_id column?\n",
    "\n",
    "datasets.to_excel(f'{main_dir}/SNT-datasets.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir  = '/Volumes/synapse/projects/SocialSpace/Projects/SNT-fmri_original/Data/SNT'\n",
    "beh_dir   = f'{base_dir}/Behavior'\n",
    "beh_files = glob.glob(beh_dir + '/*.xlsx')\n",
    "\n",
    "# if compute_rdvs:\n",
    "# snt_preprc.compute_rdvs(beh_file, metric='euclidean', output_all=False, out_dir=base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory + dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# task version. info:\n",
    "synapse_dir = '/Volumes/synapse/projects/SocialSpace/Projects/'\n",
    "task_versions = pd.read_excel(f'{synapse_dir}/SNT_datasets_task-versions.xlsx', sheet_name='CUD')\n",
    "task_versions.sort_values(by='character_role_num', inplace=True)\n",
    "\n",
    "\n",
    "# loop over datasets\n",
    "sample   = 'CUD'\n",
    "data_dir = '/Volumes/synapse/projects/SocialSpace/Projects/SNT-fmri_CUD/Data'\n",
    "info     = pd.read_excel(f'{data_dir}/SNT-task_versions.xlsx')\n",
    "\n",
    "\n",
    "#-----------------------\n",
    "# version details\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------\n",
    "# memory\n",
    "#-----------------------\n",
    "\n",
    "# should be the answers to all the questionnaires..?\n",
    "ans = ['powerful','powerful','boss','neutral','first','assistant','first','second','neutral','powerful',\n",
    "       'boss','first','neutral','assistant','assistant','neutral','second','powerful','assistant','second',\n",
    "       'neutral','second','powerful','first','boss','first','boss','assistant','second','boss'] # this will probably be the same for the ptsd sample too?\n",
    "\n",
    "mem_df = pd.read_excel(glob.glob(f'{data_dir}/SNT-memory*_raw.xlsx')[0])\n",
    "n_mem  = len(mem_df)\n",
    "if not os.path.exists(f'{data_dir}/Summary/SNT-memory_n{n_mem}.xlsx'):\n",
    "    \n",
    "    memory_df = []\n",
    "    for s,sub in mem_df.iterrows():\n",
    "\n",
    "        # get correct version\n",
    "        task_ver = task_versions[[c for c in task_versions.columns if f'v{sub.Task_ver}' in c]]\n",
    "        opts     = [x.lower() for x in task_ver[f'v{sub.Task_ver}_name']]\n",
    "     \n",
    "        # score the answers\n",
    "        resps   = np.array([x.lower() for x in list(sub.values[2:])]) \n",
    "        resps   = [snt_info.character_roles[opts.index(r)] for r in resps] # get characte role for each response\n",
    "        correct = (np.array(resps)==np.array(ans)) * 1\n",
    "        acc     = np.mean(correct)\n",
    "\n",
    "        df = pd.DataFrame([sub.Sub_id, sub.Task_ver, acc] + list(correct)).T\n",
    "        df.columns = ['sub_id','task_version','memory_mean'] + [f'memory_{r+1}_{role}' for r,role in enumerate(roles)]\n",
    "        memory_df.append(df)\n",
    "    memory_df = pd.concat(memory_df)\n",
    "    memory_df.to_excel(f'{data_dir}/Summary/SNT-memory_n{len(memory_df)}_processed.xlsx')\n",
    "\n",
    "print('Memory processing completed')\n",
    "\n",
    "\n",
    "#-----------------------\n",
    "# dots\n",
    "#-----------------------\n",
    "\n",
    "dots_dir  = f'{data_dir}/Dots'\n",
    "dots_jpgs = glob.glob(f'{dots_dir}/Dots*jpg')\n",
    "print(f'Found {len(dots_jpgs)} dots jpgs')\n",
    "\n",
    "# append to an existing dots summary if it exists\n",
    "initial_fname = glob.glob(f'{data_dir}/Summary/SNT-dots_n*.xlsx')\n",
    "if len(initial_fname) > 0: \n",
    "    dots_df  = pd.read_excel(initial_fname[0][0])\n",
    "    sub_list = dots_df['sub_id'].values\n",
    "else:\n",
    "    dots_df  = []\n",
    "    sub_list = []\n",
    "    \n",
    "print(f'{len(sub_list)} subjects dots already processed')\n",
    "for j, jpg in enumerate(dots_jpgs):\n",
    "    \n",
    "    sub_id = int(jpg.split('Dots_')[1].split('.jpg')[0])\n",
    "    if sub_id not in sub_list:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            print(f'{j+1} {jpg}', end=\"\\r\")\n",
    "            df = snt_preprc.process_dots(jpg)[1]\n",
    "            \n",
    "            # get the correct character roles for each dot\n",
    "            sub_ix   = np.where(info['sub_id'].values == sub_id)[0][0]\n",
    "            task_v   = info.loc[sub_ix, 'version'] \n",
    "            task_ver = task_versions[[c for c in task_versions.columns if f'v{task_v}' in c]]\n",
    "            chars    = [x.lower() for x in task_ver[f'v{task_v}_dots-name']]\n",
    "            for k,i in chars.items(): df.columns = df.columns.str.replace(k, i, regex=True) # replace columns\n",
    "\n",
    "            df.insert(0, 'sub_id', sub_id)\n",
    "            dots_df.append(df)\n",
    "            \n",
    "        except:\n",
    "            print(f'ERROR: {jpg}')\n",
    "\n",
    "dots_df = pd.concat(dots_df)\n",
    "dots_df.to_excel(f'{data_dir}/Summary/SNT-dots_n{len(dots_df)}.xlsx', index=False)\n",
    "if len(initial_fname) > 0: os.remove(initial_fname) # replace old w/ new\n",
    "\n",
    "print('Dots processing completed')\n",
    "\n",
    "\n",
    "#-----------------------\n",
    "# merge all\n",
    "#-----------------------\n",
    "\n",
    "summary_files = [f for f in glob.glob(f'{data_dir}/Summary/*') if ('~$' not in f) and ('All-summary') not in f]\n",
    "merged_df     = snt_utils.merge_dfs(summary_files)\n",
    "merged_df     = snt_utils.move_cols_to_front(merged_df, ['sub_id', 'dx'])\n",
    "merged_df.to_excel(f'{data_dir}/Summary/All-summary_n{len(merged_df)}.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('social_navigation_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3f7fb9d13e0b7cd346c5bcfe5de21908f17a2d0dcbd7821a2fa8e4fbf9948e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
